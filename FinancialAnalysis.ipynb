{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyP49moAfF+JvXxXEQM2VOjN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73011713cf264b0999a808265905aecf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c3d6c63ccb043c5b2b3d822bee47239","IPY_MODEL_7aa3076163104d1cba8fb343fa2d9543","IPY_MODEL_27258d717dda41d4a0b64965b00beb57"],"layout":"IPY_MODEL_139b325adbda4a81acb0b47dcab86574"}},"5c3d6c63ccb043c5b2b3d822bee47239":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa5fd42d34774225ac667bde20e4936d","placeholder":"​","style":"IPY_MODEL_2f45c0fda62d4b6392677712cd4d8e2e","value":"Loading checkpoint shards: 100%"}},"7aa3076163104d1cba8fb343fa2d9543":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4239f9b129f49e59128148fbf4220db","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85882d6f9b344952b6d6b91301e311d0","value":2}},"27258d717dda41d4a0b64965b00beb57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7b1fc3fc3874d6ea83e96f3e2265269","placeholder":"​","style":"IPY_MODEL_f11759b4256643ff84d8848d2608ec69","value":" 2/2 [00:09&lt;00:00,  4.29s/it]"}},"139b325adbda4a81acb0b47dcab86574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa5fd42d34774225ac667bde20e4936d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f45c0fda62d4b6392677712cd4d8e2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4239f9b129f49e59128148fbf4220db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85882d6f9b344952b6d6b91301e311d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7b1fc3fc3874d6ea83e96f3e2265269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f11759b4256643ff84d8848d2608ec69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Install and import libraries"],"metadata":{"id":"81XiwR8-9hGg"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5w6r5WB7MKL_","executionInfo":{"status":"ok","timestamp":1742082285209,"user_tz":240,"elapsed":2514,"user":{"displayName":"Caden Webb","userId":"00439241620104196769"}},"outputId":"a883f3bb-1209-4308-e3b3-61db9f64e725"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"]}],"source":["# Install required packages\n","!pip install faiss-cpu sentence-transformers tqdm transformers accelerate"]},{"cell_type":"code","source":["import os\n","import json\n","import glob\n","import numpy as np\n","import pandas as pd\n","import torch\n","import faiss\n","import tqdm\n","import gc\n","import time\n","import datetime\n","from typing import List, Dict, Any, Tuple, Optional\n","from dataclasses import dataclass\n","from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, AutoModel\n","from sentence_transformers import SentenceTransformer\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM"],"metadata":{"id":"kZfRpmLGAAZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7s4cU8Qvm0Lz","executionInfo":{"status":"ok","timestamp":1742082378925,"user_tz":240,"elapsed":93710,"user":{"displayName":"Caden Webb","userId":"00439241620104196769"}},"outputId":"ea6c2c29-339c-47b9-c8a9-248d3eda2306"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: fineGrained).\n","The token `caden_access` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `caden_access`\n"]}]},{"cell_type":"markdown","source":["# Configuration"],"metadata":{"id":"P6_m1d8TASOu"}},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd /content/drive/MyDrive/MSDS490_Project\n","# import os\n","# print(os.getcwd())"],"metadata":{"id":"fmMXWLMnCT7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Memory management utility functions\n","def clear_gpu_memory():\n","    \"\"\"Clear GPU memory to free up resources.\"\"\"\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","def get_gpu_memory_usage():\n","    \"\"\"Return GPU memory usage in MB.\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.cuda.memory_allocated() / 1024**2\n","    return 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuaL2N23AT3l","executionInfo":{"status":"ok","timestamp":1742082378950,"user_tz":240,"elapsed":12,"user":{"displayName":"Caden Webb","userId":"00439241620104196769"}},"outputId":"6b9abf47-5137-4326-a023-f2b1add9ba66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["# Main Code"],"metadata":{"id":"lF9aOdQ2BDau"}},{"cell_type":"markdown","source":["## Document Class"],"metadata":{"id":"M3a50RsxBLMl"}},{"cell_type":"code","source":["@dataclass\n","class Document:\n","    \"\"\"Class to represent a document in the RAG system.\"\"\"\n","    text: str\n","    metadata: Dict[str, Any]"],"metadata":{"id":"_-g-wej9BC3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Financial Data Loader"],"metadata":{"id":"ouWP9zqRBTTw"}},{"cell_type":"code","source":["class FinancialDataLoader:\n","    \"\"\"Load and preprocess financial data from various sources.\"\"\"\n","\n","    def __init__(self, base_dir: str):\n","        \"\"\"Initialize the data loader.\n","\n","        Args:\n","            base_dir: Base directory containing the data folders\n","        \"\"\"\n","        self.base_dir = base_dir\n","        self.tenk_dir = os.path.join(base_dir, \"10-K\")\n","        self.pr_dir = os.path.join(base_dir, \"PR\")\n","        self.earnings_dir = os.path.join(base_dir, \"Earnings_Transcripts\")\n","        self.sec_dir = os.path.join(base_dir, \"SEC_Data\")\n","\n","        # Get all company folders\n","        self.companies = set()\n","        for directory in [self.tenk_dir, self.pr_dir, self.earnings_dir, self.sec_dir]:\n","            if os.path.exists(directory):\n","                self.companies.update([os.path.basename(f) for f in glob.glob(os.path.join(directory, \"*\"))\n","                                      if os.path.isdir(f)])\n","        print(f\"Found {len(self.companies)} companies: {', '.join(self.companies)}\")\n","\n","    def load_10k_data(self) -> List[Document]:\n","        \"\"\"Load 10-K report data.\"\"\"\n","        documents = []\n","\n","        print(\"Loading 10-K data...\")\n","        for company in tqdm.tqdm(self.companies):\n","            company_dir = os.path.join(self.tenk_dir, company)\n","            if not os.path.exists(company_dir):\n","                continue\n","\n","            for file_path in glob.glob(os.path.join(company_dir, \"*.json\")):\n","                try:\n","                    with open(file_path, 'r') as f:\n","                        data = json.load(f)\n","\n","                    # Extract filename components for metadata\n","                    filename = os.path.basename(file_path)\n","\n","                    # Process each chunk in the file\n","                    if isinstance(data, list):\n","                        for i, chunk in enumerate(data):\n","                            if isinstance(chunk, dict) and 'text' in chunk:\n","                                doc = Document(\n","                                    text=chunk['text'],\n","                                    metadata={\n","                                        'source': '10-K',\n","                                        'company': company,\n","                                        'file': filename,\n","                                        'chunk_id': i,\n","                                        **{k: v for k, v in chunk.items() if k != 'text'}\n","                                    }\n","                                )\n","                                documents.append(doc)\n","                    elif isinstance(data, dict) and 'text' in data:\n","                        doc = Document(\n","                            text=data['text'],\n","                            metadata={\n","                                'source': '10-K',\n","                                'company': company,\n","                                'file': filename,\n","                                **{k: v for k, v in data.items() if k != 'text'}\n","                            }\n","                        )\n","                        documents.append(doc)\n","                except Exception as e:\n","                    print(f\"Error loading 10-K file {file_path}: {e}\")\n","\n","        print(f\"Loaded {len(documents)} 10-K document chunks\")\n","        return documents\n","\n","    def load_pr_data(self) -> List[Document]:\n","        \"\"\"Load press release data.\"\"\"\n","        documents = []\n","\n","        print(\"Loading press release data...\")\n","        for company in tqdm.tqdm(self.companies):\n","            company_dir = os.path.join(self.pr_dir, company)\n","            if not os.path.exists(company_dir):\n","                continue\n","\n","            for file_path in glob.glob(os.path.join(company_dir, \"*\")):\n","                try:\n","                    # For raw text files\n","                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n","                        content = f.read()\n","\n","                    # Extract date from filename if possible (assuming format like YYYY-MM-DD or similar)\n","                    filename = os.path.basename(file_path)\n","\n","                    # Create document\n","                    doc = Document(\n","                        text=content,\n","                        metadata={\n","                            'source': 'Press Release',\n","                            'company': company,\n","                            'file': filename\n","                        }\n","                    )\n","                    documents.append(doc)\n","                except Exception as e:\n","                    print(f\"Error loading PR file {file_path}: {e}\")\n","\n","        print(f\"Loaded {len(documents)} press release documents\")\n","        return documents\n","\n","    def load_earnings_data(self) -> List[Document]:\n","        \"\"\"Load earnings transcript data.\"\"\"\n","        documents = []\n","\n","        print(\"Loading earnings transcript data...\")\n","\n","        for company in tqdm.tqdm(self.companies):\n","            company_dir = os.path.join(self.earnings_dir, company)\n","            if not os.path.exists(company_dir):\n","                continue\n","\n","            for file_path in glob.glob(os.path.join(company_dir, \"*.json\")):\n","                try:\n","                    with open(file_path, 'r') as f:\n","                        data = json.load(f)\n","\n","                    # Extract filename components for metadata\n","                    filename = os.path.basename(file_path)\n","\n","                    # Process each chunk in the file\n","                    if isinstance(data, list):\n","                        for i, chunk in enumerate(data):\n","                            # Check for 'content' field instead of 'text'\n","                            if isinstance(chunk, dict) and 'content' in chunk:\n","                                # Extract metadata if available\n","                                chunk_metadata = chunk.get('metadata', {})\n","\n","                                doc = Document(\n","                                    text=chunk['content'],  # Use 'content' instead of 'text'\n","                                    metadata={\n","                                        'source': 'Earnings Transcript',\n","                                        'company': company,\n","                                        'file': filename,\n","                                        'chunk_id': i,\n","                                        # Include the chunk's metadata if available\n","                                        **chunk_metadata,\n","                                        # Include any other top-level fields except 'content'\n","                                        **{k: v for k, v in chunk.items() if k not in ['content', 'metadata']}\n","                                    }\n","                                )\n","                                documents.append(doc)\n","                    elif isinstance(data, dict):\n","                        # Handle case where the file contains a single object instead of an array\n","                        if 'content' in data:\n","                            # Extract metadata if available\n","                            doc_metadata = data.get('metadata', {})\n","\n","                            doc = Document(\n","                                text=data['content'],  # Use 'content' instead of 'text'\n","                                metadata={\n","                                    'source': 'Earnings Transcript',\n","                                    'company': company,\n","                                    'file': filename,\n","                                    # Include the document's metadata if available\n","                                    **doc_metadata,\n","                                    # Include any other top-level fields except 'content'\n","                                    **{k: v for k, v in data.items() if k not in ['content', 'metadata']}\n","                                }\n","                            )\n","                            documents.append(doc)\n","                except Exception as e:\n","                    print(f\"Error loading earnings file {file_path}: {e}\")\n","\n","        print(f\"Loaded {len(documents)} earnings transcript chunks\")\n","        return documents\n","\n","    def load_sec_data(self) -> List[Document]:\n","        \"\"\"Load SEC financial metric data.\"\"\"\n","        documents = []\n","\n","        print(\"Loading SEC financial data...\")\n","        for company in tqdm.tqdm(self.companies):\n","            company_dir = os.path.join(self.sec_dir, company)\n","            if not os.path.exists(company_dir):\n","                continue\n","\n","            for file_path in glob.glob(os.path.join(company_dir, \"*.json\")):\n","                try:\n","                    with open(file_path, 'r') as f:\n","                        data = json.load(f)\n","\n","                    # For financial metrics, transform the JSON into a more readable text format\n","                    # so it can be embedded and retrieved effectively\n","                    if isinstance(data, dict):\n","                        for metric, values in data.items():\n","                            if isinstance(values, list) and len(values) > 0:\n","                                # Group by year for better context\n","                                by_year = {}\n","                                for entry in values:\n","                                    if 'end' in entry and 'val' in entry:\n","                                        year = entry['end'].split('-')[0]\n","                                        if year not in by_year:\n","                                            by_year[year] = []\n","                                        by_year[year].append(entry)\n","\n","                                for year, entries in by_year.items():\n","                                    # Format the financial data as text\n","                                    text_content = f\"Financial Metric: {metric} for {company} in {year}\\n\\n\"\n","                                    for entry in entries:\n","                                        date = entry.get('end', 'N/A')\n","                                        value = entry.get('val', 'N/A')\n","                                        form = entry.get('form', 'N/A')\n","                                        filed = entry.get('filed', 'N/A')\n","\n","                                        text_content += f\"As of {date}, {metric} was {value:,} \"\n","                                        text_content += f\"reported in {form} filed on {filed}.\\n\"\n","\n","                                    doc = Document(\n","                                        text=text_content,\n","                                        metadata={\n","                                            'source': 'SEC Financial Data',\n","                                            'company': company,\n","                                            'metric': metric,\n","                                            'year': year,\n","                                            'file': os.path.basename(file_path)\n","                                        }\n","                                    )\n","                                    documents.append(doc)\n","                except Exception as e:\n","                    print(f\"Error loading SEC file {file_path}: {e}\")\n","\n","        print(f\"Loaded {len(documents)} SEC financial data documents\")\n","        return documents\n","\n","    def load_all_data(self) -> List[Document]:\n","        \"\"\"Load all available data.\"\"\"\n","        all_docs = []\n","\n","        # Load data from each source\n","        tenk_docs = self.load_10k_data()\n","        pr_docs = self.load_pr_data()\n","        earnings_docs = self.load_earnings_data()\n","        sec_docs = self.load_sec_data()\n","\n","        # Combine all documents\n","        all_docs.extend(tenk_docs)\n","        all_docs.extend(pr_docs)\n","        all_docs.extend(earnings_docs)\n","        all_docs.extend(sec_docs)\n","\n","        # Count documents by source\n","        source_counts = {}\n","        for doc in all_docs:\n","            source = doc.metadata.get('source', 'Unknown')\n","            if source not in source_counts:\n","                source_counts[source] = 0\n","            source_counts[source] += 1\n","\n","        # Print document counts by source\n","        print(\"\\nDocument counts by source:\")\n","        for source, count in source_counts.items():\n","            print(f\"  {source}: {count} documents\")\n","\n","        print(f\"\\nTotal documents loaded: {len(all_docs)}\")\n","        return all_docs"],"metadata":{"id":"TijXYWOkBiki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector Storage"],"metadata":{"id":"Gg0Vp606Eflq"}},{"cell_type":"code","source":["class VectorStore:\n","    \"\"\"FAISS vector store for document retrieval.\"\"\"\n","\n","    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n","        \"\"\"Initialize the vector store.\n","\n","        Args:\n","            embedding_model_name: Name of the SentenceTransformer model to use\n","        \"\"\"\n","        try:\n","            self.embedding_model = SentenceTransformer(embedding_model_name)\n","            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n","            print(f\"Loaded embedding model: {embedding_model_name} with dimension {self.embedding_dim}\")\n","        except Exception as e:\n","            print(f\"Error loading embedding model: {e}\")\n","            raise\n","\n","        self.index = None\n","        self.documents = []\n","\n","    def add_documents(self, documents: List[Document], batch_size: int = 64):\n","        \"\"\"Add documents to the vector store.\n","\n","        Args:\n","            documents: List of documents to add\n","            batch_size: Batch size for processing embeddings\n","        \"\"\"\n","        self.documents = documents\n","\n","        # Create batches for memory efficiency\n","        text_batches = [\n","            [doc.text for doc in documents[i:i+batch_size]]\n","            for i in range(0, len(documents), batch_size)\n","        ]\n","\n","        all_embeddings = []\n","        total_batches = len(text_batches)\n","\n","        print(f\"Computing embeddings for {len(documents)} documents in {total_batches} batches...\")\n","        for i, batch in enumerate(text_batches):\n","            print(f\"Processing batch {i+1}/{total_batches}...\")\n","            batch_embeddings = self.embedding_model.encode(batch, show_progress_bar=True)\n","            all_embeddings.append(batch_embeddings)\n","\n","            # Memory management\n","            if (i+1) % 10 == 0:\n","                clear_gpu_memory()\n","                print(f\"GPU memory usage: {get_gpu_memory_usage():.2f} MB\")\n","\n","        embeddings = np.vstack(all_embeddings)\n","        print(f\"Generated embeddings shape: {embeddings.shape}\")\n","\n","        # Create FAISS index\n","        self.index = faiss.IndexFlatL2(self.embedding_dim)\n","        self.index.add(embeddings.astype('float32'))\n","        print(\"FAISS index created successfully\")\n","\n","    def save(self, directory: str):\n","        \"\"\"Save the vector store to disk.\n","\n","        Args:\n","            directory: Directory to save the vector store\n","        \"\"\"\n","        os.makedirs(directory, exist_ok=True)\n","\n","        # Save FAISS index\n","        faiss.write_index(self.index, os.path.join(directory, \"index.faiss\"))\n","\n","        # Save documents as JSON\n","        with open(os.path.join(directory, \"documents.json\"), 'w') as f:\n","            json.dump([\n","                {\n","                    'text': doc.text,\n","                    'metadata': doc.metadata\n","                }\n","                for doc in self.documents\n","            ], f)\n","\n","        # Save embedding model name\n","        # Save embedding model name\n","        with open(os.path.join(directory, \"config.json\"), 'w') as f:\n","            # Store the model name directly if it was provided at initialization\n","            model_name = getattr(self.embedding_model, 'model_name', None)\n","            if model_name is None:\n","                # Try to get it from the tokenizer\n","                if hasattr(self.embedding_model, 'tokenizer') and hasattr(self.embedding_model.tokenizer, 'name_or_path'):\n","                    model_name = self.embedding_model.tokenizer.name_or_path\n","                else:\n","                    # Fallback: use the model's class name\n","                    model_name = self.embedding_model.__class__.__name__\n","\n","            json.dump({\n","                'embedding_model': model_name,\n","                'embedding_dim': self.embedding_dim\n","            }, f)\n","\n","        print(f\"Vector store saved to {directory}\")\n","\n","    @classmethod\n","    def load(cls, directory: str):\n","        \"\"\"Load a vector store from disk.\n","\n","        Args:\n","            directory: Directory containing the saved vector store\n","\n","        Returns:\n","            Loaded VectorStore instance\n","        \"\"\"\n","        # Load config\n","        with open(os.path.join(directory, \"config.json\"), 'r') as f:\n","            config = json.load(f)\n","\n","        # Create instance with saved embedding model\n","        vector_store = cls(embedding_model_name=config['embedding_model'])\n","\n","        # Load FAISS index\n","        vector_store.index = faiss.read_index(os.path.join(directory, \"index.faiss\"))\n","\n","        # Load documents\n","        with open(os.path.join(directory, \"documents.json\"), 'r') as f:\n","            docs_data = json.load(f)\n","            vector_store.documents = [\n","                Document(text=item['text'], metadata=item['metadata'])\n","                for item in docs_data\n","            ]\n","\n","        print(f\"Loaded vector store from {directory}\")\n","        print(f\"Index contains {vector_store.index.ntotal} vectors\")\n","        print(f\"Loaded {len(vector_store.documents)} documents\")\n","\n","        return vector_store\n","\n","\n","    def search(self, query: str, top_k: int = 5) -> List[Tuple[Document, float]]:\n","        \"\"\"Search for similar documents to the query.\n","\n","        Args:\n","            query: Query string\n","            top_k: Number of results to return\n","\n","        Returns:\n","            List of (document, score) tuples\n","        \"\"\"\n","        # Encode query\n","        query_embedding = self.embedding_model.encode([query])[0].reshape(1, -1).astype('float32')\n","\n","        # Search\n","        distances, indices = self.index.search(query_embedding, top_k)\n","\n","        # Return results\n","        results = [\n","            (self.documents[idx], float(distance))\n","            for idx, distance in zip(indices[0], distances[0])\n","            if idx < len(self.documents)  # Safety check for index bounds\n","        ]\n","\n","        return results\n"],"metadata":{"id":"pWd3ynipD5Dp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prompt Builder"],"metadata":{"id":"ZPK2fuiAFMxe"}},{"cell_type":"code","source":["class PromptBuilder:\n","    \"\"\"Build prompts for different audience types.\"\"\"\n","\n","    AUDIENCE_TEMPLATES = {\n","        \"analyst\": {\n","            \"system\": \"\"\"You are an AI assistant specialized in financial analysis. You analyze corporate financial data, SEC filings, earnings transcripts, and press releases to provide detailed analytical insights. Focus on trends, metrics, comparative analysis, and factual reporting. Use precise financial terminology and cite your sources clearly.\"\"\",\n","            \"human\": \"\"\"Analyze the following information about {companies} focused on {query}. Provide a comprehensive analysis with specific metrics, trends, and risk factors. Include inline citations to the source documents for all claims.\n","\n","Context information:\n","{context}\"\"\"\n","        },\n","        \"executive\": {\n","            \"system\": \"\"\"You are an AI assistant that provides concise executive summaries of financial information. You analyze corporate financial data, SEC filings, earnings transcripts, and press releases to extract strategic insights. Focus on high-level implications, market positioning, and business impact. Be direct, concise, and action-oriented in your analysis.\"\"\",\n","            \"human\": \"\"\"Provide an executive summary about {companies} focused on {query}. Highlight key strategic insights, competitive positioning, and business implications. Be concise but thorough, and include inline citations to the source documents.\n","\n","Context information:\n","{context}\"\"\"\n","        },\n","        \"investor\": {\n","            \"system\": \"\"\"You are an AI assistant that analyzes financial information for investors. You review corporate financial data, SEC filings, earnings transcripts, and press releases to provide investment-focused insights. Focus on performance metrics, future outlook, risk assessment, and potential investment implications. Be balanced in your assessment and cite your sources clearly.\"\"\",\n","            \"human\": \"\"\"Provide an investor-focused analysis of {companies} regarding {query}. Address performance trends, growth potential, risk factors, and competitive position. Include relevant metrics and inline citations to the source documents.\n","\n","Context information:\n","{context}\"\"\"\n","        }\n","    }\n","\n","    @classmethod\n","    def build_prompt(cls, query: str, retrieved_docs: List[Tuple[Document, float]],\n","                    audience: str = \"analyst\") -> Tuple[str, str]:\n","        \"\"\"Build a prompt for the given query, retrieved documents, and audience type.\n","\n","        Args:\n","            query: User query\n","            retrieved_docs: List of (document, score) tuples from vector search\n","            audience: Type of audience (analyst, executive, or investor)\n","\n","        Returns:\n","            Tuple of (system_prompt, user_prompt)\n","        \"\"\"\n","        if audience not in cls.AUDIENCE_TEMPLATES:\n","            audience = \"analyst\"  # Default to analyst\n","\n","        # Extract companies from the retrieved documents\n","        companies = set()\n","        for doc, _ in retrieved_docs:\n","            if 'company' in doc.metadata:\n","                companies.add(doc.metadata['company'])\n","\n","        companies_str = \", \".join(sorted(companies)) if companies else \"the companies\"\n","\n","        # Format context with citations\n","        context_parts = []\n","        for i, (doc, score) in enumerate(retrieved_docs):\n","            # Create a citation identifier\n","            source_type = doc.metadata.get('source', 'Unknown')\n","            company = doc.metadata.get('company', 'Unknown')\n","\n","            # Format date information if available\n","            date_info = \"\"\n","            if 'file' in doc.metadata:\n","                # Try to extract date from filename if it exists\n","                filename = doc.metadata['file']\n","                # Simple pattern matching for dates in filenames (can be enhanced)\n","                date_parts = [part for part in filename.split('_') if part.isdigit() and len(part) == 4]\n","                if date_parts:\n","                    date_info = f\" ({date_parts[0]})\"\n","                elif 'year' in doc.metadata:\n","                    date_info = f\" ({doc.metadata['year']})\"\n","\n","            citation = f\"[{source_type} - {company}{date_info}]\"\n","\n","            # Add the document text with citation\n","            context_parts.append(f\"{doc.text}\\n\\n{citation}\")\n","\n","        context = \"\\n\\n\".join(context_parts)\n","\n","        # Get templates for the audience\n","        templates = cls.AUDIENCE_TEMPLATES[audience]\n","        system_prompt = templates[\"system\"]\n","        human_prompt = templates[\"human\"].format(\n","            companies=companies_str,\n","            query=query,\n","            context=context\n","        )\n","\n","        return system_prompt, human_prompt"],"metadata":{"id":"rPSmvBdaFOeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Mistral Model"],"metadata":{"id":"n_ZjzXUiFTPi"}},{"cell_type":"code","source":["class MistralModel:\n","    \"\"\"Wrapper for Mistral model to generate responses.\"\"\"\n","\n","    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\",\n","                max_length: int = 2048, device_map: str = \"auto\"):\n","        \"\"\"Initialize the Mistral model.\n","\n","        Args:\n","            model_name: Name of the Mistral model to load\n","            max_length: Maximum length of generated text\n","            device_map: Device mapping strategy for model loading\n","        \"\"\"\n","        try:\n","            print(f\"Loading Mistral model: {model_name}\")\n","            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                model_name,\n","                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","                device_map=device_map,\n","                low_cpu_mem_usage=True\n","            )\n","            self.max_length = max_length\n","            print(f\"Model loaded successfully on {self.model.device}\")\n","        except Exception as e:\n","            print(f\"Error loading model: {e}\")\n","            raise\n","\n","    def generate(self, system_prompt: str, user_prompt: str) -> str:\n","        \"\"\"Generate a response for the given prompts.\n","\n","        Args:\n","            system_prompt: System prompt\n","            user_prompt: User prompt\n","\n","        Returns:\n","            Generated response\n","        \"\"\"\n","        # Format prompt for Mistral\n","        prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_prompt} [/INST]\"\n","\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n","\n","        # Calculate and print token count\n","        input_token_count = inputs.input_ids.shape[1]\n","        print(f\"Input token count: {input_token_count}\")\n","\n","        # You can also print an estimated character-to-token ratio for future reference\n","        char_to_token_ratio = len(prompt) / input_token_count\n","        print(f\"Character-to-token ratio: {char_to_token_ratio:.2f} characters per token\")\n","        print(f\"Total prompt length: {len(prompt)} characters\")\n","\n","        # Clear memory before generation\n","        clear_gpu_memory()\n","        print(f\"GPU memory before generation: {get_gpu_memory_usage():.2f} MB\")\n","\n","        # Generate response\n","        with torch.no_grad():\n","            output = self.model.generate(\n","                **inputs,\n","                #max_length=self.max_length,\n","                max_new_tokens=700,\n","                temperature=0.7,\n","                top_p=0.9,\n","                repetition_penalty=1.1,\n","                do_sample=True\n","            )\n","\n","        # Process output\n","        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n","        # Check if response contains the instruction end tag\n","        if \"[/INST]\" in response:\n","            # Split at the instruction end tag and take everything after it\n","            response = response.split(\"[/INST]\", 1)[1].strip()\n","        else:\n","            # Fallback to string replacement if tag not found\n","            response = response.replace(prompt, \"\").strip()\n","\n","        # Clean up memory\n","        del inputs, output\n","        clear_gpu_memory()\n","        print(f\"GPU memory after generation: {get_gpu_memory_usage():.2f} MB\")\n","\n","        return response"],"metadata":{"id":"dm5u4kPDFUvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logger"],"metadata":{"id":"R4guWz-S212S"}},{"cell_type":"code","source":["class Logger:\n","    \"\"\"Log system interactions to text files.\"\"\"\n","\n","    def __init__(self, log_dir: str):\n","        \"\"\"Initialize the logger.\n","\n","        Args:\n","            log_dir: Directory to store log files\n","        \"\"\"\n","        self.log_dir = log_dir\n","        os.makedirs(log_dir, exist_ok=True)\n","\n","        # Create a unique log file name with timestamp\n","        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","        self.log_file = os.path.join(log_dir, f\"chat_log_{current_time}.txt\")\n","\n","        # Initialize the log file with a header\n","        with open(self.log_file, 'w') as f:\n","            f.write(f\"=== FINANCIAL ANALYSIS SYSTEM LOG ===\\n\")\n","            f.write(f\"Started: {current_time}\\n\")\n","            f.write(f\"{'='*40}\\n\\n\")\n","\n","    def log_interaction(self, query: str, audience: str, system_prompt: str,\n","                        user_prompt: str, response: str, context_docs: List,\n","                        response_time: float, max_context_chars: int = 2000):\n","        \"\"\"Log a complete interaction to the log file.\n","\n","        Args:\n","            query: The user's query\n","            audience: The selected audience type\n","            system_prompt: The system prompt used\n","            user_prompt: The user prompt generated\n","            response: The model's response\n","            context_docs: The retrieved context documents\n","            response_time: Time taken to generate response (seconds)\n","            max_context_chars: Maximum characters to include from context\n","        \"\"\"\n","        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","        # Format context for logging (truncated if too long)\n","        context_str = self._format_context(context_docs, max_context_chars)\n","\n","        with open(self.log_file, 'a') as f:\n","            # Write timestamp and query details\n","            f.write(f\"TIMESTAMP: {current_time}\\n\")\n","            f.write(f\"QUERY: {query}\\n\")\n","            f.write(f\"AUDIENCE: {audience}\\n\")\n","            f.write(f\"RESPONSE TIME: {response_time:.2f} seconds\\n\\n\")\n","\n","            # Write system prompt\n","            f.write(\"SYSTEM PROMPT:\\n\")\n","            f.write(f\"{system_prompt}\\n\\n\")\n","\n","            # Write context (truncated if needed)\n","            f.write(\"CONTEXT (RETRIEVED DOCUMENTS):\\n\")\n","            f.write(f\"{context_str}\\n\\n\")\n","\n","            # Write user prompt and response\n","            f.write(\"USER PROMPT:\\n\")\n","            f.write(f\"{user_prompt}\\n\\n\")\n","\n","            f.write(\"RESPONSE:\\n\")\n","            f.write(f\"{response}\\n\\n\")\n","\n","            # Add separator for readability\n","            f.write(f\"{'='*80}\\n\\n\")\n","\n","    def _format_context(self, context_docs: List, max_chars: int) -> str:\n","        \"\"\"Format context documents for logging, with truncation.\n","\n","        Args:\n","            context_docs: List of (Document, score) tuples\n","            max_chars: Maximum characters to include\n","\n","        Returns:\n","            Formatted context string\n","        \"\"\"\n","        context_parts = []\n","        total_chars = 0\n","        truncated = False\n","\n","        for i, (doc, score) in enumerate(context_docs):\n","            # Create a citation identifier\n","            source_type = doc.metadata.get('source', 'Unknown')\n","            company = doc.metadata.get('company', 'Unknown')\n","\n","            # Extract date info if available\n","            date_info = \"\"\n","            if 'file' in doc.metadata:\n","                filename = doc.metadata['file']\n","                date_parts = [part for part in filename.split('_') if part.isdigit() and len(part) == 4]\n","                if date_parts:\n","                    date_info = f\"({date_parts[0]})\"\n","                elif 'year' in doc.metadata:\n","                    date_info = f\"({doc.metadata['year']})\"\n","\n","            # Create document summary\n","            doc_summary = f\"[Doc {i+1}] {source_type} - {company} {date_info} (Relevance: {score:.4f})\"\n","            doc_text = doc.text\n","\n","            # Check if adding this document would exceed the max chars\n","            additional_chars = len(doc_summary) + len(doc_text) + 20  # 20 for formatting\n","\n","            if total_chars + additional_chars > max_chars and i > 0:\n","                truncated = True\n","                break\n","\n","            # Add this document\n","            context_parts.append(f\"{doc_summary}\\n{doc_text[:500]}...\\n\")\n","            total_chars += additional_chars\n","\n","        # Add truncation notice if needed\n","        if truncated:\n","            context_parts.append(f\"\\n[... {len(context_docs) - i} more documents truncated for readability ...]\\n\")\n","\n","        return \"\\n\".join(context_parts)"],"metadata":{"id":"7LMPaj_I24X0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analysis System"],"metadata":{"id":"1Y3Nkg6OFcZI"}},{"cell_type":"code","source":["class FinancialAnalysisSystem:\n","    \"\"\"Main class for the financial analysis system.\"\"\"\n","\n","    def __init__(self, base_dir: str, vector_store_dir: str = None):\n","        \"\"\"Initialize the financial analysis system.\n","\n","        Args:\n","            base_dir: Base directory containing the data folders\n","            vector_store_dir: Directory to load/save the vector store\n","        \"\"\"\n","        self.base_dir = base_dir\n","        self.vector_store_dir = vector_store_dir\n","        self.vector_store = None\n","        self.model = None\n","\n","    def initialize(self, rebuild_vector_store: bool = False,\n","                  model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\"):\n","        \"\"\"Initialize the system components.\n","\n","        Args:\n","            rebuild_vector_store: Whether to rebuild the vector store\n","            model_name: Name of the model to load\n","        \"\"\"\n","        # Initialize vector store\n","        if self.vector_store_dir and os.path.exists(self.vector_store_dir) and not rebuild_vector_store:\n","            print(f\"Loading existing vector store from {self.vector_store_dir}\")\n","            self.vector_store = VectorStore.load(self.vector_store_dir)\n","        else:\n","            print(\"Building new vector store\")\n","            # Load data\n","            data_loader = FinancialDataLoader(self.base_dir)\n","            documents = data_loader.load_all_data()\n","\n","            # Create vector store\n","            self.vector_store = VectorStore()\n","            self.vector_store.add_documents(documents)\n","\n","            # Save vector store if directory is specified\n","            if self.vector_store_dir:\n","                print(f\"Saving vector store to {self.vector_store_dir}\")\n","                self.vector_store.save(self.vector_store_dir)\n","\n","        # Initialize Mistral model\n","        print(f\"Initializing Mistral model: {model_name}\")\n","        self.model = MistralModel(model_name=model_name)\n","\n","        print(\"System initialization complete\")\n","\n","    def query(self, query: str, audience: str = \"analyst\", top_k: int = 10) -> str:\n","        \"\"\"Process a query and generate a response.\n","\n","        Args:\n","            query: User query\n","            audience: Type of audience (analyst, executive, or investor)\n","            top_k: Number of documents to retrieve\n","\n","        Returns:\n","            Generated response\n","        \"\"\"\n","        if not self.vector_store or not self.model:\n","            raise ValueError(\"System not initialized. Call initialize() first.\")\n","\n","        print(f\"Processing query: '{query}' for audience: {audience}\")\n","\n","        # Retrieve relevant documents\n","        print(f\"Retrieving top {top_k} documents...\")\n","        retrieved_docs = self.vector_store.search(query, top_k=top_k)\n","\n","        # Build prompt\n","        print(\"Building prompt...\")\n","        system_prompt, user_prompt = PromptBuilder.build_prompt(\n","            query=query,\n","            retrieved_docs=retrieved_docs,\n","            audience=audience\n","        )\n","\n","        # Generate response\n","        print(\"Generating response...\")\n","        response = self.model.generate(system_prompt, user_prompt)\n","\n","        return response"],"metadata":{"id":"7V4d4-WQFeRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Enhanced FinancialAnalysisSystem with logging\n","class EnhancedFinancialAnalysisSystem(FinancialAnalysisSystem):\n","    \"\"\"Enhanced Financial Analysis System with logging capabilities.\"\"\"\n","\n","    def __init__(self, base_dir: str, vector_store_dir: str = None):\n","        \"\"\"Initialize the financial analysis system with logging.\n","\n","        Args:\n","            base_dir: Base directory containing the data folders\n","            vector_store_dir: Directory to load/save the vector store\n","        \"\"\"\n","        super().__init__(base_dir, vector_store_dir)\n","\n","        # Create logger\n","        log_dir = os.path.join(base_dir, \"Chat_Logs\")\n","        self.logger = Logger(log_dir)\n","        print(f\"Logging enabled. Logs will be saved to: {log_dir}\")\n","\n","    def query(self, query: str, audience: str = \"analyst\", top_k: int = 10) -> str:\n","        \"\"\"Process a query and generate a response with logging.\n","\n","        Args:\n","            query: User query\n","            audience: Type of audience (analyst, executive, or investor)\n","            top_k: Number of documents to retrieve\n","\n","        Returns:\n","            Generated response\n","        \"\"\"\n","        if not self.vector_store or not self.model:\n","            raise ValueError(\"System not initialized. Call initialize() first.\")\n","\n","        print(f\"Processing query: '{query}' for audience: {audience}\")\n","\n","        # Record start time\n","        start_time = time.time()\n","\n","        # Retrieve relevant documents\n","        print(f\"Retrieving top {top_k} documents...\")\n","        retrieved_docs = self.vector_store.search(query, top_k=top_k)\n","\n","        # Build prompt\n","        print(\"Building prompt...\")\n","        system_prompt, user_prompt = PromptBuilder.build_prompt(\n","            query=query,\n","            retrieved_docs=retrieved_docs,\n","            audience=audience\n","        )\n","\n","        # Generate response\n","        print(\"Generating response...\")\n","        response = self.model.generate(system_prompt, user_prompt)\n","\n","        # Calculate response time\n","        response_time = time.time() - start_time\n","        print(f\"Response generated in {response_time:.2f} seconds\")\n","\n","        # Log the interaction\n","        self.logger.log_interaction(\n","            query=query,\n","            audience=audience,\n","            system_prompt=system_prompt,\n","            user_prompt=user_prompt,\n","            response=response,\n","            context_docs=retrieved_docs,\n","            response_time=response_time\n","        )\n","\n","        return response\n","\n"],"metadata":{"id":"eoyzGVePE5o1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run system"],"metadata":{"id":"xf2X0PtP_iS4"}},{"cell_type":"code","source":["# Example usage function with enhanced system\n","def run_enhanced_financial_analysis_system():\n","    \"\"\"Run the enhanced financial analysis system with logging.\"\"\"\n","\n","    # Mount Google Drive (required for Colab)\n","    try:\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        base_dir = \"/content/drive/MyDrive/MSDS490_Project\"\n","    except:\n","        # Fallback for non-Colab environments\n","        base_dir = os.getcwd()\n","\n","    vector_store_dir = os.path.join(base_dir, \"vector_store\")\n","\n","    print(\"Initializing Enhanced Financial Analysis System with Logging...\")\n","    system = EnhancedFinancialAnalysisSystem(base_dir, vector_store_dir)\n","\n","    # Initialize with or without rebuilding the vector store\n","    rebuild = input(\"Rebuild vector store? (y/n): \").lower() == 'y'\n","\n","    # Select model\n","    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Default\n","    model_selection = input(\"Select model (1: Mistral-7B-Instruct, 2: TinyLlama-1.1B): \")\n","    if model_selection == \"2\":\n","        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","    system.initialize(rebuild_vector_store=rebuild, model_name=model_name)\n","\n","    # Interactive query loop\n","    while True:\n","        query = input(\"\\nEnter your query (or 'quit' to exit): \")\n","        if query.lower() == 'quit':\n","            break\n","\n","        audience = input(\"Select audience (1: Analyst, 2: Executive, 3: Investor): \")\n","        audience_map = {\n","            \"1\": \"analyst\",\n","            \"2\": \"executive\",\n","            \"3\": \"investor\"\n","        }\n","        audience_type = audience_map.get(audience, \"analyst\")\n","\n","        top_k = int(input(\"Number of documents to retrieve (3-20): \") or \"10\")\n","        top_k = max(3, min(20, top_k))  # Constrain between 3 and 20\n","\n","        print(\"\\nProcessing your query...\\n\")\n","        response = system.query(query, audience=audience_type, top_k=top_k)\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"RESPONSE:\")\n","        print(\"=\"*80)\n","        print(response)\n","        print(\"=\"*80)"],"metadata":{"id":"Oe_MkLF5E9TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Example usage function\n","# def run_financial_analysis_system():\n","#     \"\"\"Run the financial analysis system with example queries.\"\"\"\n","\n","#     # Mount Google Drive (required for Colab)\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","\n","#     # Initialize the system\n","#     base_dir = \"/content/drive/MyDrive/MSDS490_Project\"\n","#     vector_store_dir = \"/content/drive/MyDrive/MSDS490_Project/vector_store\"\n","\n","#     print(\"Initializing Financial Analysis System...\")\n","#     system = FinancialAnalysisSystem(base_dir, vector_store_dir)\n","\n","#     # Initialize with or without rebuilding the vector store\n","#     rebuild = input(\"Rebuild vector store? (y/n): \").lower() == 'y'\n","\n","#     # Select model\n","#     # Select model\n","#     model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Default\n","#     model_selection = input(\"Select model (1: Mistral-7B-Instruct, 2: TinyLlama-1.1B): \")\n","#     if model_selection == \"2\":\n","#         model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","#     system.initialize(rebuild_vector_store=rebuild, model_name=model_name)\n","\n","#     # Interactive query loop\n","#     while True:\n","#         query = input(\"\\nEnter your query (or 'quit' to exit): \")\n","#         if query.lower() == 'quit':\n","#             break\n","\n","#         audience = input(\"Select audience (1: Analyst, 2: Executive, 3: Investor): \")\n","#         audience_map = {\n","#             \"1\": \"analyst\",\n","#             \"2\": \"executive\",\n","#             \"3\": \"investor\"\n","#         }\n","#         audience_type = audience_map.get(audience, \"analyst\")\n","\n","#         top_k = int(input(\"Number of documents to retrieve (5-20): \") or \"10\")\n","#         top_k = max(5, min(20, top_k))  # Constrain between 5 and 20\n","\n","#         print(\"\\nProcessing your query...\\n\")\n","#         response = system.query(query, audience=audience_type, top_k=top_k)\n","\n","#         print(\"\\n\" + \"=\"*80)\n","#         print(\"RESPONSE:\")\n","#         print(\"=\"*80)\n","#         print(response)\n","#         print(\"=\"*80)"],"metadata":{"id":"rxG0ST7AFmDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Execute the enhanced system\n","if __name__ == \"__main__\":\n","    run_enhanced_financial_analysis_system()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":989,"referenced_widgets":["73011713cf264b0999a808265905aecf","5c3d6c63ccb043c5b2b3d822bee47239","7aa3076163104d1cba8fb343fa2d9543","27258d717dda41d4a0b64965b00beb57","139b325adbda4a81acb0b47dcab86574","fa5fd42d34774225ac667bde20e4936d","2f45c0fda62d4b6392677712cd4d8e2e","e4239f9b129f49e59128148fbf4220db","85882d6f9b344952b6d6b91301e311d0","d7b1fc3fc3874d6ea83e96f3e2265269","f11759b4256643ff84d8848d2608ec69"]},"id":"QxLnKAqvFB_S","executionInfo":{"status":"ok","timestamp":1742082579702,"user_tz":240,"elapsed":200570,"user":{"displayName":"Caden Webb","userId":"00439241620104196769"}},"outputId":"474a79b8-f950-4c41-8344-f7d86c6c709c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Initializing Enhanced Financial Analysis System with Logging...\n","Logging enabled. Logs will be saved to: /content/drive/MyDrive/MSDS490_Project/Chat_Logs\n","Rebuild vector store? (y/n): n\n","Select model (1: Mistral-7B-Instruct, 2: TinyLlama-1.1B): 1\n","Loading existing vector store from /content/drive/MyDrive/MSDS490_Project/vector_store\n","Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2 with dimension 384\n","Loaded vector store from /content/drive/MyDrive/MSDS490_Project/vector_store\n","Index contains 119936 vectors\n","Loaded 119936 documents\n","Initializing Mistral model: mistralai/Mistral-7B-Instruct-v0.1\n","Loading Mistral model: mistralai/Mistral-7B-Instruct-v0.1\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73011713cf264b0999a808265905aecf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully on cuda:0\n","System initialization complete\n","\n","Enter your query (or 'quit' to exit): Based on future growth projections, would it be better to pursue a job at RTX or Tesla?\n","Select audience (1: Analyst, 2: Executive, 3: Investor): 3\n","Number of documents to retrieve (3-20): 3\n","\n","Processing your query...\n","\n","Processing query: 'Based on future growth projections, would it be better to pursue a job at RTX or Tesla?' for audience: investor\n","Retrieving top 3 documents...\n","Building prompt...\n","Generating response...\n","Input token count: 637\n","Character-to-token ratio: 3.71 characters per token\n","Total prompt length: 2364 characters\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["GPU memory before generation: 13242.27 MB\n","GPU memory after generation: 13242.26 MB\n","Response generated in 111.15 seconds\n","\n","================================================================================\n","RESPONSE:\n","================================================================================\n","Based on the available data, both RTX and TSLA have shown promising growth trends, but there are differences in their performance metrics, future outlook, risk factors, and competitive position.\n","\n","Performance Metrics:\n","RTX has outperformed its peers in terms of local U.S. job creation in 2022. According to the Russell 1000, RTX created 8,967 jobs, which is significantly more than any other company in the same index. Additionally, RTX ranks first in employee-giving and volunteering, indicating a positive impact on the communities where they operate. On the other hand, TSLA reported a significant increase in revenue, with net sales increasing by 47% YoY to $15.3 billion in Q3 2022. However, their gross margin decreased from 25.2% to 19.5%, indicating higher production costs.\n","\n","Future Outlook:\n","Both RTX and TSLA have expressed optimism about their future growth prospects. RTX plans to continue investing in innovation, including developing new products and services, expanding their global presence, and increasing employee engagement through volunteer programs and community initiatives. TSLA, on the other hand, expects to see continued growth in their automotive business, driven by increased demand for sustainable vehicles. They also anticipate expansion into new markets, such as energy storage and renewable energy solutions.\n","\n","Risk Factors:\n","Both companies face several risks that could impact their future performance. RTX operates in a highly competitive industry, with many established players and emerging technologies disrupting traditional business models. TSLA faces similar challenges, including regulatory hurdles and competition from established automakers. Additionally, both companies are exposed to supply chain disruptions, geopolitical tensions, and economic downturns.\n","\n","Investment Implications:\n","Based on the available data, investors should carefully consider the strengths and weaknesses of each company before making a decision. RTX's focus on job creation and employee engagement may appeal to socially responsible investors, while TSLA's innovative approach to sustainable transportation may attract those looking for high-growth opportunities. Ultimately, the choice between the two companies will depend on an individual's investment goals, risk tolerance, and values.\n","================================================================================\n","\n","Enter your query (or 'quit' to exit): quit\n"]}]},{"cell_type":"code","source":["# Import the main module\n","# from financial_analysis_rag import run_financial_analysis_system\n","\n","# Execute the system\n","# if __name__ == \"__main__\":\n","#     run_financial_analysis_system()"],"metadata":{"id":"lgmFtCYS_j0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JmTClfJG8f8f"},"execution_count":null,"outputs":[]}]}