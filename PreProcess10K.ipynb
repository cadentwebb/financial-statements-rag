{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPcKuvJbBK9jSKJbcjB7jKw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIbB2NAT9eBr","executionInfo":{"status":"ok","timestamp":1742079266395,"user_tz":240,"elapsed":794973,"user":{"displayName":"Caden Webb","userId":"00439241620104196769"}},"outputId":"632e38ca-c8ec-4dad-9dda-7bf6f1f015ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Scanning directories for 10-K reports...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [01:05<00:00,  5.91s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Found 11 report files with a total of 2520606 chunks\n","Global reduction factor: 0.0198\n","Processing 11 companies...\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 1/11 [00:49<08:10, 49.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3489 chunks from aapl_10k_rag.json (AAPL, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 2/11 [01:36<07:15, 48.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3327 chunks from amzn_10k_rag.json (AMZN, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 3/11 [02:34<07:00, 52.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 4141 chunks from googl_10k_rag.json (GOOGL, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 4/11 [05:10<10:53, 93.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 10985 chunks from jpm_10k_rag.json (JPM, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 5/11 [05:56<07:37, 76.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3215 chunks from meta_10k_rag.json (META, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▍    | 6/11 [06:45<05:35, 67.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3417 chunks from msft_10k_rag.json (MSFT, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▎   | 7/11 [07:44<04:17, 64.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 4148 chunks from nvda_10k_rag.json (NVDA, 2025)\n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 8/11 [08:39<03:04, 61.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3864 chunks from rtx_10k_rag.json (RTX, 2019)\n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 9/11 [09:45<02:05, 62.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 4605 chunks from tsla_10k_rag.json (TSLA, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 10/11 [11:00<01:06, 66.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 5380 chunks from v_10k_rag.json (V, 2024)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11/11 [11:49<00:00, 64.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Kept 3425 chunks from wmt_10k_rag.json (WMT, 2025)\n","Total chunks kept: 49996\n","\n","===== Summary Report =====\n","Original chunks: 2520606\n","Chunks after reduction: 49996\n","Reduction: 98.02%\n","Processing time: 12.91 minutes\n","Summary saved to /content/drive/MyDrive/MSDS490_Project/10-K_Cleaned/reduction_summary.json\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# 10-K Report Pre-Processing\n","# This notebook reduces the number of chunks in 10-K reports to a more manageable size\n","# while maintaining the most relevant information across all available reports.\n","\n","import os\n","import json\n","import glob\n","import re\n","import tqdm\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from concurrent.futures import ProcessPoolExecutor, as_completed\n","import time\n","import gc\n","from typing import List, Dict, Any, Tuple, Optional\n","\n","# Mount Google Drive (uncomment for Google Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Configuration\n","BASE_DIR = \"/content/drive/MyDrive/MSDS490_Project\"\n","TENK_DIR = os.path.join(BASE_DIR, \"10-K\")\n","OUTPUT_DIR = os.path.join(BASE_DIR, \"10-K_Cleaned\")\n","TARGET_CHUNKS = 50000  # Target number of chunks to keep\n","MAX_WORKERS = 4  # Adjust based on your available resources\n","\n","# Create output directory\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Helper functions\n","\n","def count_chunks_in_file(file_path):\n","    \"\"\"Count the number of chunks in a JSON file.\"\"\"\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","            if isinstance(data, list):\n","                return len(data)\n","            return 1\n","    except Exception as e:\n","        print(f\"Error counting chunks in {file_path}: {e}\")\n","        return 0\n","\n","def get_report_metadata(file_path):\n","    \"\"\"Extract metadata from the file path and its content.\"\"\"\n","    try:\n","        filename = os.path.basename(file_path)\n","        company = os.path.basename(os.path.dirname(file_path))\n","\n","        # Try to extract year from filename\n","        year_match = re.search(r'(\\d{4})', filename)\n","        year = year_match.group(1) if year_match else \"unknown\"\n","\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        # Check the first item for metadata\n","        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n","            metadata = data[0].get('metadata', {})\n","            year = metadata.get('year', year)\n","            filing_number = metadata.get('filing_number', 0)\n","        else:\n","            filing_number = 0\n","\n","        return {\n","            'company': company,\n","            'filename': filename,\n","            'year': year,\n","            'filing_number': filing_number\n","        }\n","    except Exception as e:\n","        print(f\"Error extracting metadata from {file_path}: {e}\")\n","        return {\n","            'company': os.path.basename(os.path.dirname(file_path)),\n","            'filename': os.path.basename(file_path),\n","            'year': \"unknown\",\n","            'filing_number': 0\n","        }\n","\n","def collect_report_info():\n","    \"\"\"Collect information about all 10-K report files.\"\"\"\n","    report_files = []\n","\n","    print(\"Scanning directories for 10-K reports...\")\n","    companies = [d for d in os.listdir(TENK_DIR) if os.path.isdir(os.path.join(TENK_DIR, d))]\n","\n","    total_chunks = 0\n","    for company in tqdm.tqdm(companies):\n","        company_dir = os.path.join(TENK_DIR, company)\n","        json_files = glob.glob(os.path.join(company_dir, \"*_rag.json\"))\n","\n","        for file_path in json_files:\n","            num_chunks = count_chunks_in_file(file_path)\n","            total_chunks += num_chunks\n","\n","            if num_chunks > 0:\n","                metadata = get_report_metadata(file_path)\n","                report_files.append({\n","                    'path': file_path,\n","                    'num_chunks': num_chunks,\n","                    **metadata\n","                })\n","\n","    print(f\"Found {len(report_files)} report files with a total of {total_chunks} chunks\")\n","    return report_files, total_chunks\n","\n","def is_important_section(text):\n","    \"\"\"Identify if a chunk is from an important section of the 10-K report.\"\"\"\n","    important_keywords = [\n","        \"risk factors\", \"management discussion\", \"financial statements\",\n","        \"MD&A\", \"business overview\", \"executive summary\", \"results of operations\",\n","        \"liquidity\", \"capital resources\", \"critical accounting\", \"market risk\",\n","        \"revenue\", \"profit\", \"income\", \"expense\", \"cash flow\", \"assets\",\n","        \"liabilities\", \"debt\", \"equity\", \"dividend\", \"acquisition\", \"strategy\",\n","        \"outlook\", \"guidance\", \"forecast\"\n","    ]\n","\n","    # Check if text contains any important keywords\n","    text_lower = text.lower()\n","    return any(keyword in text_lower for keyword in important_keywords)\n","\n","def calculate_chunk_importance(chunks):\n","    \"\"\"Calculate importance score for each chunk based on content.\"\"\"\n","    # Extract text content\n","    texts = [chunk.get('text', '') for chunk in chunks]\n","\n","    # Initialize importance scores\n","    importance_scores = np.zeros(len(texts))\n","\n","    # 1. Score based on important section keywords\n","    for i, text in enumerate(texts):\n","        if is_important_section(text):\n","            importance_scores[i] += 2.0\n","\n","    # 2. Score based on TF-IDF (identifies unique/distinctive chunks)\n","    try:\n","        # Create TF-IDF matrix\n","        vectorizer = TfidfVectorizer(\n","            max_features=5000,\n","            stop_words='english',\n","            min_df=2,\n","            max_df=0.95\n","        )\n","        tfidf_matrix = vectorizer.fit_transform(texts)\n","\n","        # Calculate average TF-IDF score for each document\n","        tfidf_scores = np.array([tfidf_matrix[i].mean() for i in range(len(texts))])\n","\n","        # Normalize and add to importance scores\n","        if tfidf_scores.max() > 0:\n","            normalized_tfidf = (tfidf_scores - tfidf_scores.min()) / (tfidf_scores.max() - tfidf_scores.min())\n","            importance_scores += normalized_tfidf\n","    except Exception as e:\n","        print(f\"Error calculating TF-IDF: {e}\")\n","\n","    # 3. Score based on chunk length (longer chunks may contain more information)\n","    text_lengths = np.array([len(text) for text in texts])\n","    if text_lengths.max() > 0:\n","        length_scores = (text_lengths - text_lengths.min()) / (text_lengths.max() - text_lengths.min())\n","        importance_scores += 0.5 * length_scores\n","\n","    return importance_scores\n","\n","def process_file(file_path, reduction_factor):\n","    \"\"\"Process a single 10-K report file and select important chunks.\"\"\"\n","    try:\n","        with open(file_path, 'r') as f:\n","            chunks = json.load(f)\n","\n","        if not isinstance(chunks, list):\n","            chunks = [chunks]\n","\n","        # If the file has too few chunks, keep all of them\n","        if len(chunks) <= 10:\n","            return chunks\n","\n","        # Calculate importance scores\n","        importance_scores = calculate_chunk_importance(chunks)\n","\n","        # Determine how many chunks to keep\n","        num_to_keep = max(10, int(len(chunks) * reduction_factor))\n","\n","        # Create array of indices sorted by importance score\n","        sorted_indices = np.argsort(-importance_scores)\n","\n","        # Select the most important chunks\n","        selected_indices = sorted_indices[:num_to_keep]\n","\n","        # Sort selected indices to maintain original order\n","        selected_indices = np.sort(selected_indices)\n","\n","        # Extract selected chunks\n","        selected_chunks = [chunks[i] for i in selected_indices]\n","\n","        return selected_chunks\n","    except Exception as e:\n","        print(f\"Error processing file {file_path}: {e}\")\n","        return []\n","\n","def process_all_files(report_files, total_chunks):\n","    \"\"\"Process all 10-K report files in parallel.\"\"\"\n","    # Calculate global reduction factor\n","    global_reduction_factor = TARGET_CHUNKS / total_chunks\n","    print(f\"Global reduction factor: {global_reduction_factor:.4f}\")\n","\n","    processed_data = {}\n","    total_chunks_kept = 0\n","\n","    # Group files by company for better organization\n","    companies = {}\n","    for report in report_files:\n","        company = report['company']\n","        if company not in companies:\n","            companies[company] = []\n","        companies[company].append(report)\n","\n","    # Process each company's files\n","    print(f\"Processing {len(companies)} companies...\")\n","    for company_name, company_reports in tqdm.tqdm(companies.items()):\n","        company_dir = os.path.join(OUTPUT_DIR, company_name)\n","        os.makedirs(company_dir, exist_ok=True)\n","\n","        # Adjusted reduction factor for newer reports\n","        # Sort reports by filing number (latest reports first)\n","        company_reports.sort(key=lambda x: x['filing_number'], reverse=True)\n","\n","        # Process files in parallel\n","        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n","            futures = {}\n","\n","            for i, report in enumerate(company_reports):\n","                # Adjust reduction factor based on report age\n","                # Keep more chunks from recent reports, fewer from older ones\n","                age_factor = 1.0 if i < 2 else (0.8 if i < 4 else 0.6)\n","                adjusted_factor = global_reduction_factor * age_factor\n","\n","                future = executor.submit(process_file, report['path'], adjusted_factor)\n","                futures[future] = report\n","\n","            # Collect results\n","            for future in as_completed(futures):\n","                report = futures[future]\n","                try:\n","                    selected_chunks = future.result()\n","\n","                    if selected_chunks:\n","                        # Save to new file\n","                        output_path = os.path.join(company_dir, f\"{os.path.splitext(report['filename'])[0]}_cleaned.json\")\n","                        with open(output_path, 'w') as f:\n","                            json.dump(selected_chunks, f)\n","\n","                        total_chunks_kept += len(selected_chunks)\n","                        print(f\"Kept {len(selected_chunks)} chunks from {report['filename']} ({report['company']}, {report['year']})\")\n","                except Exception as e:\n","                    print(f\"Error processing {report['path']}: {e}\")\n","\n","    print(f\"Total chunks kept: {total_chunks_kept}\")\n","    return total_chunks_kept\n","\n","# Main execution\n","def main():\n","    start_time = time.time()\n","\n","    # Step 1: Collect information about all report files\n","    report_files, total_chunks = collect_report_info()\n","\n","    # Step 2: Process and reduce all files\n","    total_chunks_kept = process_all_files(report_files, total_chunks)\n","\n","    # Step 3: Generate summary report\n","    reduction_percent = (1 - (total_chunks_kept / total_chunks)) * 100\n","\n","    print(\"\\n===== Summary Report =====\")\n","    print(f\"Original chunks: {total_chunks}\")\n","    print(f\"Chunks after reduction: {total_chunks_kept}\")\n","    print(f\"Reduction: {reduction_percent:.2f}%\")\n","    print(f\"Processing time: {(time.time() - start_time) / 60:.2f} minutes\")\n","\n","    # Create summary file\n","    summary = {\n","        \"original_chunks\": total_chunks,\n","        \"reduced_chunks\": total_chunks_kept,\n","        \"reduction_percent\": reduction_percent,\n","        \"processing_time_minutes\": (time.time() - start_time) / 60\n","    }\n","\n","    with open(os.path.join(OUTPUT_DIR, \"reduction_summary.json\"), 'w') as f:\n","        json.dump(summary, f, indent=2)\n","\n","    print(f\"Summary saved to {os.path.join(OUTPUT_DIR, 'reduction_summary.json')}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}